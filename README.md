# ğŸ¤– Self-Healing RAG Agent with LangGraph

A Self-Healing Retrieval-Augmented Generation (RAG) Agent built with LangGraph, Qdrant, Hugging Face embeddings, and LLM-as-a-Judge evaluation. <br>
This project demonstrates how to design an agentic RAG system that can:

* Detect retrieval failures
* Diagnose why the answer is bad
* Adapt its retrieval strategy automatically
* Retry intelligently until the answer quality improves

## ğŸš€ Key Features

* Agentic control flow with LangGraph
* Vector search using Qdrant
* Dense embeddings + cross-encoder reranking
* LLM-as-a-Judge scoring (faithfulness + relevance)
* Automatic self-healing retries
* Explainable healing trace
* Interactive Streamlit UI
* PDF-based document ingestion

## ğŸ§  What Is â€œSelf-Healing RAGâ€?

Traditional RAG systems retrieve documents once and hope for the best.<br>
This agent instead:

* Retrieves documents
* Generates an answer
* Uses an LLM to judge the answer
* Diagnoses the failure reason:
  * irrelevant_docs
  * missing_context
* Applies a targeted fix:
  * increase retrieval budget
  * enable reranking
* Retries until the answer quality improves or a retry limit is reached
* This mimics how a human retrieval engineer would debug a failing RAG system.

## ğŸ§© Architecture Overview

``` pgsql
User Query
    â†“
Retrieve Documents
    â†“
Generate Answer
    â†“
LLM-as-a-Judge (Score + Failure Reason)
    â†“
Should Retry?
   â”œâ”€ No â†’ END
   â””â”€ Yes
        â†“
   Healing Strategy
        â†“
   Increment Retry Count
        â†“
   Retrieve Again (Improved)
```

## ğŸ§  Failure Modes Detected

| Failure Reason | Meaning | Healing Action |
| --- | --- | --- |
| irrelevant_docs | Retrieved docs donâ€™t match query | Enable reranking + increase budget |
| missing_context | Docs are related but insufficient | Increase retrieval budget + rerank |
| none | Answer is good | Stop execution |

## ğŸ“‚ Project Structure

```bash
Self-Healing-RAG/
â”‚
â”œâ”€â”€ langgraph_agent/
â”‚   â”œâ”€â”€ document_loader.py   # PDF loading + chunking
â”‚   â”œâ”€â”€ retrieve_docs.py     # Embedding, retrieval, reranking, LLM judge
â”‚   â”œâ”€â”€ nodes.py             # LangGraph nodes + state definition
â”‚   â””â”€â”€ graph.py             # LangGraph control flow
â”‚
â”œâ”€â”€ app.py                   # Streamlit application
â””â”€â”€ README.md
```

## ğŸ› ï¸ Tech Stack

* LangGraph â€“ agentic workflows
* LangChain â€“ document loading & splitting
* Qdrant â€“ vector database
* FastEmbed â€“ Hugging Face embeddings
* Cross-Encoder Reranker â€“ relevance refinement
* OpenAI GPT-4o â€“ answer generation
* OpenAI GPT-3.5 â€“ LLM-as-a-Judge
* Streamlit â€“ UI

### Dependencies

* `fastembed` >= 0.7.4
* `hf-xet` >= 1.2.0
* `ipykernel` >= 7.1.0
* `langchain-community` >= 0.4.1
* `langchain-huggingface` >= 1.2.0
* `langchain-text-splitters` >= 1.1.0
* `langgraph` >= 1.0.5
* `numpy` >= 2.4.0
* `openai` >= 2.14.0
* `pypdf` >= 6.5.0
* `qdrant-client` >= 1.16.2
* `sentence-transformers` >= 5.2.0
* `streamlit` >= 1.52

## ğŸ§ª How the LLM-as-a-Judge Works

The evaluator LLM receives:

* User query
* Retrieved documents
* Generated answer

It returns structured JSON:

```json
{
  "relevant_docs": true,
  "sufficient_context": false,
  "score": 0.62
}
```


This output directly controls the agentâ€™s next action.

ğŸ§¾ State Managed by the Agent
class RAGState(TypedDict):
    text: List[str]
    query: str
    retrieved_docs: List[str]
    retrieval_mode: str
    retrieval_budget: int
    answer: str
    score: float
    failure_reason: str
    retry_count: int
    max_retries: int
    healing_trace: List[str]

This explicit state design makes the system:

* debuggable
* explainable
* extensible

## â–¶ï¸ Running the App

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

(Ensure you have qdrant-client, fastembed, langgraph, streamlit, and openai installed.)

### 2ï¸âƒ£ Run Streamlit

```bash
streamlit run app.py
```

### 3ï¸âƒ£ Usage

1. Enter your OpenAI API key
2. Upload a PDF document
3. Ask questions about the document
4. Watch the agent:

* retrieve
* evaluate
* self-heal
* retry

## ğŸ” Example Healing Trace

```yaml
Irrelevant docs â†’ enabled rerank + increased retrieval budget by 2
Missing context â†’ increased retrieval budget by 3 + rerank
```

This makes the agentâ€™s reasoning transparent and inspectable.

## ğŸ¯ Why This Project Matters

This repo demonstrates real-world RAG engineering, not toy demos:

* Explicit failure modeling
* Adaptive retrieval strategies
* Agentic decision-making
* LLMs used for control, not just generation

It is ideal for:

* Portfolio projects
* RAG system interviews
* Agentic AI experimentation
* Production design discussions

## ğŸ”® Possible Extensions

* Query rewriting
* Hallucination detection
* Hybrid BM25 + dense retrieval
* Multi-document routing
* LangSmith tracing
* Evaluation dataset logging

## ğŸ‘¤ Author

Designed with â¤ï¸ by Gustavo R. Santos<br>
ğŸ”— https://gustavorsantos.me

## License

Project licensed under the MIT License.

---

## Observations

### 1. A Little About LangGraph

LangGraph = *â€œA directed graph where nodes mutate shared state, and edges depend on that state.â€*

#### Core Mental Model (Burn This In)

* Every LangGraph project answers four questions:
* What is my state?
* Which nodes modify which parts of the state?
* Which decisions depend on the state?
* When does the graph stop?
